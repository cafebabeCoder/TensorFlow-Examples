{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import zipfile\n",
    "import urllib\n",
    "import os\n",
    "import collections\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# data\n",
    "\n",
    "url = 'http://mattmahoney.net/dc/text8.zip'\n",
    "data_path = '/tmp/text8.zip'\n",
    "if not os.path.exists(data_path):\n",
    "    name, _ = urllib.request.urlretrieve(url)\n",
    "    print('download')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unzip\n",
    "with zipfile.ZipFile(data_path) as z:\n",
    "    text_words = z.read(z.namelist()[0]).lower().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 1000\n",
    "min_cur = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('UNK', 15350134), (b'the', 1061396), (b'of', 593677)]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = [('UNK', -1)]\n",
    "count.extend(collections.Counter(text_words).most_common(max_len - 1))\n",
    "\n",
    "for i in range(len(count)-1, -1, -1):\n",
    "    if count[i][1]<min_cur:\n",
    "        count.pop(i)\n",
    "    else:\n",
    "        break\n",
    "\n",
    "vocabulary_size = len(count)\n",
    "word2id = dict()\n",
    "for i, (w, _) in enumerate(count):\n",
    "    word2id[w] = i\n",
    "    \n",
    "data=[]\n",
    "unk = 0\n",
    "for w in text_words:\n",
    "    ind = word2id.get(w, 0)\n",
    "    if ind == 0:\n",
    "        unk += 1\n",
    "    data.append(ind)\n",
    "    \n",
    "count[0]=('UNK', unk)\n",
    "id2word = dict(zip(word2id.values(), word2id.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_index = 0\n",
    "# Generate training batch for the skip-gram model. skip_gram 是一推多\n",
    "def next_batch(batch_size, num_skips, skip_window):\n",
    "    global data_index\n",
    "    assert batch_size % num_skips == 0\n",
    "    assert num_skips <= 2 * skip_window\n",
    "    batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "    labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "    # get window size (words left and right + current one).\n",
    "    span = 2 * skip_window + 1\n",
    "    buffer = collections.deque(maxlen=span)\n",
    "    if data_index + span > len(data):\n",
    "        data_index = 0\n",
    "#     print(data_index)\n",
    "    buffer.extend(data[data_index:data_index + span])\n",
    "#     print(buffer)\n",
    "    data_index += span\n",
    "    for i in range(batch_size // num_skips):\n",
    "        context_words = [w for w in range(span) if w != skip_window]\n",
    "        words_to_use = random.sample(context_words, num_skips)\n",
    "        for j, context_word in enumerate(words_to_use):\n",
    "            batch[i * num_skips + j] = buffer[skip_window]\n",
    "            labels[i * num_skips + j, 0] = buffer[context_word]\n",
    "        if data_index == len(data):\n",
    "            buffer.extend(data[0:span])\n",
    "            data_index = span\n",
    "        else:\n",
    "            buffer.append(data[data_index])\n",
    "            data_index += 1\n",
    "#         print(\"----\\n\", i)\n",
    "#         print(buffer)\n",
    "#         print(data_index)\n",
    "#         print(batch)\n",
    "#         print(labels)\n",
    "        \n",
    "    # Backtrack a little bit to avoid skipping words in the end of a batch.\n",
    "    data_index = (data_index + len(data) - span) % len(data)\n",
    "    return batch, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = 16\n",
    "num_sampled = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "embedding = tf.Variable(tf.random.normal([vocabulary_size, dim]))\n",
    "nce_weight = tf.Variable(tf.random.normal([vocabulary_size, dim]))\n",
    "nce_biase = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "def lookup_embedding(w):\n",
    "    return tf.nn.embedding_lookup(embedding, w)\n",
    "\n",
    "# loss\n",
    "def nce_loss(logits, y):\n",
    "    y = tf.cast(y, tf.int64)\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.nce_loss(\n",
    "            weights=nce_weight,\n",
    "            biases=nce_biase,\n",
    "            labels=y,\n",
    "            inputs=logits,\n",
    "            num_sampled= num_sampled,\n",
    "            num_classes=vocabulary_size))\n",
    "    return loss\n",
    "\n",
    "# eva\n",
    "def evaluate(x_emb, y):\n",
    "    logits = tf.matmul(x_emb, nce_weight, transpose_b=True)\n",
    "    logits = tf.add(logits, nce_biase)\n",
    "    y_ont_hot =  tf.squeeze(tf.one_hot(y, vocabulary_size), axis=1)\n",
    "    print('logits', logits.shape, 'y', y_ont_hot.shape)\n",
    "    loss = tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "    labels=y_ont_hot,\n",
    "    logits=logits)\n",
    "    loss = tf.reduce_mean(tf.reduce_sum(loss, axis=-1))\n",
    "    \n",
    "    #cosine\n",
    "    x_embed = tf.cast(x_emb, tf.float32)\n",
    "    x_embed_norm = x_embed/tf.sqrt(tf.reduce_sum(tf.square(x_embed)))\n",
    "    embedding_norm = embedding/tf.sqrt(tf.reduce_sum(tf.square(embedding), 1, keepdims=True), tf.float32)\n",
    "    cosine = tf.matmul(x_embed, embedding_norm, transpose_b=True)\n",
    "    return loss, cosine\n",
    "\n",
    "#optimizer\n",
    "optimizer=tf.optimizers.SGD(0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(x, y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        x_emb = lookup_embedding(x)\n",
    "        loss = nce_loss(x_emb, y)\n",
    "        print('losss', loss)\n",
    "        \n",
    "    grads = tape.gradient(loss, [embedding, nce_weight, nce_biase])\n",
    "    optimizer.apply_gradients(zip(grads, [embedding, nce_weight, nce_biase]))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "num_skips = 2\n",
    "skip_windows = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "losss tf.Tensor(19.069523, shape=(), dtype=float32)\n",
      "train loss: tf.Tensor(19.069523, shape=(), dtype=float32)\n",
      "test loss: tf.Tensor(19.020855, shape=(), dtype=float32)\n",
      "logits (64, 1000) y (64, 1000)\n",
      "loss tf.Tensor(1792.5254, shape=(), dtype=float32) cos tf.Tensor(\n",
      "[[ 4.2837744e+00  1.2748723e+00 -1.0769682e+00 ...  4.0832368e-01\n",
      "  -6.1532879e-01 -2.4855095e-01]\n",
      " [ 4.2837744e+00  1.2748723e+00 -1.0769682e+00 ...  4.0832368e-01\n",
      "  -6.1532879e-01 -2.4855095e-01]\n",
      " [-1.8320662e+00 -1.7449287e+00 -7.1548325e-01 ...  1.4244804e-01\n",
      "   5.1303184e-01 -5.6311756e-01]\n",
      " ...\n",
      " [-1.1685427e+00 -1.6191188e+00 -3.5412398e-01 ...  1.2623150e+00\n",
      "  -4.4060358e-01 -8.9376330e-02]\n",
      " [ 4.1598555e-01  1.9969952e-01  4.8482502e-01 ... -9.2368358e-01\n",
      "  -1.5941393e-01 -2.8106570e-03]\n",
      " [ 4.1598555e-01  1.9969952e-01  4.8482502e-01 ... -9.2368358e-01\n",
      "  -1.5941393e-01 -2.8106570e-03]], shape=(64, 1000), dtype=float32)\n",
      "losss tf.Tensor(24.662834, shape=(), dtype=float32)\n",
      "train loss: tf.Tensor(24.662834, shape=(), dtype=float32)\n",
      "test loss: tf.Tensor(17.953686, shape=(), dtype=float32)\n",
      "logits (64, 1000) y (64, 1000)\n",
      "loss tf.Tensor(1815.8274, shape=(), dtype=float32) cos tf.Tensor(\n",
      "[[ 4.2673264   1.2652353  -1.0634632  ...  0.4043858  -0.61057657\n",
      "  -0.2385875 ]\n",
      " [ 4.2673264   1.2652353  -1.0634632  ...  0.4043858  -0.61057657\n",
      "  -0.2385875 ]\n",
      " [-1.1914202  -1.1569428  -1.2862816  ...  0.06263578 -3.71864\n",
      "   1.498121  ]\n",
      " ...\n",
      " [ 4.2673264   1.2652353  -1.0634632  ...  0.4043858  -0.61057657\n",
      "  -0.2385875 ]\n",
      " [ 0.47625822  0.39250845  0.31690717 ...  0.11280769 -0.00831926\n",
      "  -0.6215096 ]\n",
      " [ 0.47625822  0.39250845  0.31690717 ...  0.11280769 -0.00831926\n",
      "  -0.6215096 ]], shape=(64, 1000), dtype=float32)\n",
      "losss tf.Tensor(13.409542, shape=(), dtype=float32)\n",
      "train loss: tf.Tensor(13.409542, shape=(), dtype=float32)\n",
      "test loss: tf.Tensor(16.854465, shape=(), dtype=float32)\n",
      "logits (64, 1000) y (64, 1000)\n",
      "loss tf.Tensor(1791.5112, shape=(), dtype=float32) cos tf.Tensor(\n",
      "[[ 0.6953444  -1.2000343  -1.1207082  ...  1.525748    0.12784888\n",
      "  -1.5661718 ]\n",
      " [ 0.6953444  -1.2000343  -1.1207082  ...  1.525748    0.12784888\n",
      "  -1.5661718 ]\n",
      " [ 1.3412868   4.5221133   0.58156157 ... -0.4287972   1.1860858\n",
      "   1.3643953 ]\n",
      " ...\n",
      " [ 1.3412868   4.5221133   0.58156157 ... -0.4287972   1.1860858\n",
      "   1.3643953 ]\n",
      " [ 4.266343    1.2654239  -1.0643121  ...  0.406502   -0.61317766\n",
      "  -0.23589969]\n",
      " [ 4.266343    1.2654239  -1.0643121  ...  0.406502   -0.61317766\n",
      "  -0.23589969]], shape=(64, 1000), dtype=float32)\n",
      "losss tf.Tensor(19.67912, shape=(), dtype=float32)\n",
      "train loss: tf.Tensor(19.67912, shape=(), dtype=float32)\n",
      "test loss: tf.Tensor(19.953133, shape=(), dtype=float32)\n",
      "logits (64, 1000) y (64, 1000)\n",
      "loss tf.Tensor(1768.25, shape=(), dtype=float32) cos tf.Tensor(\n",
      "[[ 1.2349663   0.12852244  0.46339023 ... -0.06057607 -0.41151226\n",
      "   0.39005202]\n",
      " [ 1.2349663   0.12852244  0.46339023 ... -0.06057607 -0.41151226\n",
      "   0.39005202]\n",
      " [-1.0231593   0.5261245   4.1212783  ... -1.4801459   1.1156385\n",
      "   0.54055285]\n",
      " ...\n",
      " [-0.72508126  0.9203088   0.35620326 ... -0.53170055  0.9811815\n",
      "  -0.21353997]\n",
      " [-1.1658843   0.664578    0.7802969  ... -1.803303    0.11881995\n",
      "   0.6385753 ]\n",
      " [-1.1658843   0.664578    0.7802969  ... -1.803303    0.11881995\n",
      "   0.6385753 ]], shape=(64, 1000), dtype=float32)\n",
      "losss tf.Tensor(21.010168, shape=(), dtype=float32)\n",
      "train loss: tf.Tensor(21.010168, shape=(), dtype=float32)\n",
      "test loss: tf.Tensor(15.110796, shape=(), dtype=float32)\n",
      "logits (64, 1000) y (64, 1000)\n",
      "loss tf.Tensor(1804.6368, shape=(), dtype=float32) cos tf.Tensor(\n",
      "[[ 4.256176    1.2713579  -1.0503988  ...  0.37250525 -0.61493284\n",
      "  -0.22800809]\n",
      " [ 4.256176    1.2713579  -1.0503988  ...  0.37250525 -0.61493284\n",
      "  -0.22800809]\n",
      " [-0.05420947  1.4931723   1.4735245  ... -0.998999    0.22416422\n",
      "   0.36371273]\n",
      " ...\n",
      " [-2.5061135  -0.4831133   0.53438205 ... -0.6786584  -0.0642304\n",
      "   0.8698067 ]\n",
      " [ 4.2561765   1.2713579  -1.0503987  ...  0.37250525 -0.6149328\n",
      "  -0.22800833]\n",
      " [ 4.2561765   1.2713579  -1.0503987  ...  0.37250525 -0.6149328\n",
      "  -0.22800833]], shape=(64, 1000), dtype=float32)\n",
      "losss tf.Tensor(17.289604, shape=(), dtype=float32)\n",
      "train loss: tf.Tensor(17.289604, shape=(), dtype=float32)\n",
      "test loss: tf.Tensor(16.699902, shape=(), dtype=float32)\n",
      "logits (64, 1000) y (64, 1000)\n",
      "loss tf.Tensor(1843.3223, shape=(), dtype=float32) cos tf.Tensor(\n",
      "[[ 0.02706167 -0.19239356 -0.60338587 ...  0.990097   -0.08320802\n",
      "  -0.23704657]\n",
      " [ 0.02706167 -0.19239356 -0.60338587 ...  0.990097   -0.08320802\n",
      "  -0.23704657]\n",
      " [ 4.2550826   1.278221   -1.04319    ...  0.37152007 -0.6155125\n",
      "  -0.2289992 ]\n",
      " ...\n",
      " [-0.04811435  1.4937572   1.4740303  ... -0.99833     0.22394177\n",
      "   0.36324927]\n",
      " [ 0.02706167 -0.19239357 -0.6033859  ...  0.99009705 -0.08320805\n",
      "  -0.23704663]\n",
      " [ 0.02706167 -0.19239357 -0.6033859  ...  0.99009705 -0.08320805\n",
      "  -0.23704663]], shape=(64, 1000), dtype=float32)\n",
      "losss tf.Tensor(16.598938, shape=(), dtype=float32)\n",
      "train loss: tf.Tensor(16.598938, shape=(), dtype=float32)\n",
      "test loss: tf.Tensor(28.741016, shape=(), dtype=float32)\n",
      "logits (64, 1000) y (64, 1000)\n",
      "loss tf.Tensor(1837.8674, shape=(), dtype=float32) cos tf.Tensor(\n",
      "[[ 1.0882058   0.9691728   0.11556799 ... -0.16514099  0.59905523\n",
      "   0.7994796 ]\n",
      " [ 1.0882058   0.9691728   0.11556799 ... -0.16514099  0.59905523\n",
      "   0.7994796 ]\n",
      " [ 0.03750192  1.0605266  -0.39711756 ...  1.5024725   1.2865338\n",
      "  -0.4988439 ]\n",
      " ...\n",
      " [ 4.2530823   1.2776885  -1.0370241  ...  0.3645121  -0.6158681\n",
      "  -0.2302053 ]\n",
      " [ 4.2530823   1.2776884  -1.037024   ...  0.36451215 -0.6158681\n",
      "  -0.2302053 ]\n",
      " [ 4.2530823   1.2776884  -1.037024   ...  0.36451215 -0.6158681\n",
      "  -0.2302053 ]], shape=(64, 1000), dtype=float32)\n",
      "losss tf.Tensor(20.87535, shape=(), dtype=float32)\n",
      "train loss: tf.Tensor(20.87535, shape=(), dtype=float32)\n",
      "test loss: tf.Tensor(13.345001, shape=(), dtype=float32)\n",
      "logits (64, 1000) y (64, 1000)\n",
      "loss tf.Tensor(1840.2842, shape=(), dtype=float32) cos tf.Tensor(\n",
      "[[-0.21835442 -1.2969723  -2.0136352  ...  1.2815022   1.2004848\n",
      "  -0.96452683]\n",
      " [-0.21835442 -1.2969723  -2.0136352  ...  1.2815022   1.2004848\n",
      "  -0.96452683]\n",
      " [-0.21835442 -1.2969723  -2.0136352  ...  1.2815022   1.2004848\n",
      "  -0.96452683]\n",
      " ...\n",
      " [ 4.247561    1.2763267  -1.0282623  ...  0.35946897 -0.6209048\n",
      "  -0.22444594]\n",
      " [-0.03626007  1.4936618   1.4740541  ... -0.9990284   0.22384824\n",
      "   0.36293286]\n",
      " [-0.03626007  1.4936618   1.4740541  ... -0.9990284   0.22384824\n",
      "   0.36293286]], shape=(64, 1000), dtype=float32)\n",
      "losss tf.Tensor(17.496273, shape=(), dtype=float32)\n",
      "train loss: tf.Tensor(17.496273, shape=(), dtype=float32)\n",
      "test loss: tf.Tensor(13.4613495, shape=(), dtype=float32)\n",
      "logits (64, 1000) y (64, 1000)\n",
      "loss tf.Tensor(1794.126, shape=(), dtype=float32) cos tf.Tensor(\n",
      "[[ 1.3587589   4.5178103   0.5772587  ... -0.4280324   1.184313\n",
      "   1.3617024 ]\n",
      " [ 1.3587589   4.5178103   0.5772587  ... -0.4280324   1.184313\n",
      "   1.3617024 ]\n",
      " [ 0.87583196 -0.72040576  0.17785215 ...  0.26020637  0.23546939\n",
      "  -0.85669124]\n",
      " ...\n",
      " [ 0.4513366   0.10592165 -0.4136189  ...  0.7146808   0.59532505\n",
      "  -1.7493432 ]\n",
      " [ 0.3365112   0.59798706  1.4743515  ...  0.7514076   1.9250767\n",
      "   0.28205764]\n",
      " [ 0.3365112   0.59798706  1.4743515  ...  0.7514076   1.9250767\n",
      "   0.28205764]], shape=(64, 1000), dtype=float32)\n",
      "losss tf.Tensor(23.589624, shape=(), dtype=float32)\n",
      "train loss: tf.Tensor(23.589624, shape=(), dtype=float32)\n",
      "test loss: tf.Tensor(15.901416, shape=(), dtype=float32)\n",
      "logits (64, 1000) y (64, 1000)\n",
      "loss tf.Tensor(1776.3433, shape=(), dtype=float32) cos tf.Tensor(\n",
      "[[ 4.2407207   1.2760884  -1.0393575  ...  0.3601521  -0.62490594\n",
      "  -0.20643502]\n",
      " [ 4.2407207   1.2760884  -1.0393575  ...  0.3601521  -0.62490594\n",
      "  -0.20643502]\n",
      " [ 4.2407207   1.2760884  -1.0393575  ...  0.3601521  -0.62490594\n",
      "  -0.20643502]\n",
      " ...\n",
      " [ 4.2407207   1.2760884  -1.0393575  ...  0.3601521  -0.62490594\n",
      "  -0.20643502]\n",
      " [-1.0092912   0.526179    4.1180453  ... -1.4771954   1.1127193\n",
      "   0.54119664]\n",
      " [-1.0092912   0.526179    4.1180453  ... -1.4771954   1.1127193\n",
      "   0.54119664]], shape=(64, 1000), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    (x_train, y_train) = next_batch(batch_size, num_skips, skip_windows)\n",
    "    loss = train_step(x_train, y_train)\n",
    "    print('train loss:', loss)\n",
    "    \n",
    "    (x_test, y_test) = next_batch(batch_size, num_skips, skip_windows)\n",
    "    x_emb = lookup_embedding(x_test)\n",
    "    loss = nce_loss(x_emb, y_test)\n",
    "    print('test loss:', loss)\n",
    "    \n",
    "    loss, cos = evaluate(x_emb, y_test)\n",
    "    print('loss', loss, 'cos', cos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
